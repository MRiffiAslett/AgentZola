[Wed 17 Dec 16:57:09 GMT 2025] Starting WhiteFox job on node: gpuvm13.doc.ic.ac.uk
SLURM job ID: 212718
CUDA_VISIBLE_DEVICES=0

/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-12-17 17:00:48.907144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Configuration:
{'config': PosixPath('xilo_xla/config/generator.toml'), 'only_opt': None}
INFO 12-17 17:03:25 [utils.py:253] non-default args: {'download_dir': '/vol/bitbucket/mtr25/AgentZola/WhiteFox/hf_cache', 'dtype': 'float16', 'seed': None, 'max_model_len': 4096, 'swap_space': 20, 'gpu_memory_utilization': 0.95, 'disable_log_stats': True, 'model': 'ise-uiuc/Magicoder-S-DS-6.7B'}
WARNING 12-17 17:03:25 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
INFO 12-17 17:03:27 [model.py:637] Resolved architecture: LlamaForCausalLM
INFO 12-17 17:03:27 [model.py:2086] Downcasting torch.float32 to torch.float16.
INFO 12-17 17:03:27 [model.py:1750] Using max model len 4096
INFO 12-17 17:03:40 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:03:43 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='ise-uiuc/Magicoder-S-DS-6.7B', speculative_config=None, tokenizer='ise-uiuc/Magicoder-S-DS-6.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir='/vol/bitbucket/mtr25/AgentZola/WhiteFox/hf_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=ise-uiuc/Magicoder-S-DS-6.7B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:03:59 [fa_utils.py:72] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:04:16 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://146.169.47.164:57865 backend=nccl
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:04:17 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:04:19 [gpu_model_runner.py:3467] Starting to load model ise-uiuc/Magicoder-S-DS-6.7B...
[0;36m(EngineCore_DP0 pid=802644)[0;0m /vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=802644)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=802644)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:05:00 [cuda.py:411] Using FLASHINFER attention backend out of potential backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=802644)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=802644)[0;0m Loading safetensors checkpoint shards:  17% Completed | 1/6 [01:47<08:59, 107.86s/it]
[0;36m(EngineCore_DP0 pid=802644)[0;0m Loading safetensors checkpoint shards:  33% Completed | 2/6 [02:36<04:53, 73.26s/it]
[0;36m(EngineCore_DP0 pid=802644)[0;0m  Loading safetensors checkpoint shards:  50% Completed | 3/6 [04:01<03:55, 78.34s/it]
[0;36m(EngineCore_DP0 pid=802644)[0;0m Loading safetensors checkpoint shards:  67% Completed | 4/6 [05:55<03:05, 92.61s/it]
[0;36m(EngineCore_DP0 pid=802644)[0;0m Loading safetensors checkpoint shards:  83% Completed | 5/6 [07:23<01:30, 90.77s/it]
[0;36m(EngineCore_DP0 pid=802644)[0;0m Loading safetensors checkpoint shards: 100% Completed | 6/6 [08:41<00:00, 86.56s/it]
[0;36m(EngineCore_DP0 pid=802644)[0;0m Loading safetensors checkpoint shards: 100% Completed | 6/6 [08:41<00:00, 86.94s/it]
[0;36m(EngineCore_DP0 pid=802644)[0;0m 
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:13:45 [default_loader.py:308] Loading weights took 521.73 seconds
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:13:45 [gpu_model_runner.py:3549] Model loading took 12.5709 GiB memory and 565.506788 seconds
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:15:19 [backends.py:655] Using cache directory: /homes/mtr25/.cache/vllm/torch_compile_cache/d0b481e00d/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:15:19 [backends.py:715] Dynamo bytecode transform time: 92.95 s
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:15:21 [backends.py:257] Cache the graph for dynamic shape for later use
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:15:25 [backends.py:288] Compiling a graph for dynamic shape takes 5.91 s
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:15:31 [monitor.py:34] torch.compile takes 98.87 s in total
[0;36m(EngineCore_DP0 pid=802644)[0;0m INFO 12-17 17:15:42 [gpu_worker.py:359] Available KV cache memory: 0.53 GiB
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 834, in run_engine_core
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 610, in __init__
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]     super().__init__(
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 243, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1335, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 707, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843]     raise ValueError(
[0;36m(EngineCore_DP0 pid=802644)[0;0m ERROR 12-17 17:15:42 [core.py:843] ValueError: To serve at least one request with the models's max seq len (4096), (2.00 GiB KV cache is needed, which is larger than the available KV cache memory (0.53 GiB). Based on the available memory, the estimated maximum model length is 1088. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[0;36m(EngineCore_DP0 pid=802644)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=802644)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=802644)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=802644)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 847, in run_engine_core
[0;36m(EngineCore_DP0 pid=802644)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 834, in run_engine_core
[0;36m(EngineCore_DP0 pid=802644)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=802644)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 610, in __init__
[0;36m(EngineCore_DP0 pid=802644)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=802644)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=802644)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 243, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=802644)[0;0m     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=802644)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1335, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=802644)[0;0m     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=802644)[0;0m   File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 707, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=802644)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=802644)[0;0m ValueError: To serve at least one request with the models's max seq len (4096), (2.00 GiB KV cache is needed, which is larger than the available KV cache memory (0.53 GiB). Based on the available memory, the estimated maximum model length is 1088. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
Traceback (most recent call last):
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/generation/main.py", line 70, in <module>
    main()
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/generation/main.py", line 44, in main
    generator = StarCoderGenerator.from_config_file(args.config)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/generation/generator.py", line 70, in from_config_file
    return cls(config)
           ^^^^^^^^^^^
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/generation/generator.py", line 50, in __init__
    self.llm = self._initialize_llm()
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/generation/generator.py", line 94, in _initialize_llm
    return LLM(
           ^^^^
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 334, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 642, in __init__
    super().__init__(
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 471, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/vol/bitbucket/mtr25/AgentZola/WhiteFox/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
