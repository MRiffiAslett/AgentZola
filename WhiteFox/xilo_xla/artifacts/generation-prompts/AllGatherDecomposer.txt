### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `AllGatherDecomposer` in TensorFlow XLA.

# Description
To trigger the `AllGatherDecomposer` optimization pass in TensorFlow XLA, the TensorFlow model must have specific characteristics related to its use of the `AllGather` operation. Here are the detailed characteristics:

1. **Presence of `AllGather` Operations:**
   The model needs to include at least one `AllGather` operation. This operation collects data across multiple devices or replicas, typically used for synchronizing tensors in distributed training.

   ```python
   import tensorflow as tf

   # Example TensorFlow model code snippet using `all_gather` operation
   strategy = tf.distribute.MirroredStrategy()
   with strategy.scope():
       tensor = tf.constant([1.0, 2.0])
       gathered = strategy.experimental_local_results(strategy.experimental_all_gather(tensor))
   ```

2. **Conditions for Decomposition:**
   The `AllGather` operation must meet criteria checked by the `ShouldDecompose` function. While the specifics of this function are not included, common reasons for decomposition might include:
   - Optimization for specific device arrangements or configurations.
   - Handling specific cases where direct all-gather might be less efficient or not supported due to hardware constraints.

3. **Multiple Operands:**
   The `AllGather` operation in the model should potentially handle multiple operands. The optimization decomposes multi-operand gathers into per-operand reductions, possibly to leverage efficiencies in how data is managed across distributed devices.

   ```python
   # Hypothetical scenario where multiple tensors are gathered
   tensor1 = tf.constant([1.0, 2.0])
   tensor2 = tf.constant([3.0, 4.0])
   gathered1, gathered2 = some_distributed_operation(tensor1, tensor2)
   ```

4. **Specific Characteristics for Decomposition:**
   - The `AllGather` operation must have a specific configuration related to `replica_groups`, `constrain_layout`, `channel_id`, and `use_global_device_ids`. These configurations can dictate specific requirements, like whether all devices globally or only within certain groups need to be synchronized, which can impact the decision to decompose.

5. **Optimization for Execution Efficiency:**
   The decomposition might be carried out to transform the `AllGather` into `AllReduce` operations, which are generally more amenable to certain types of parallel optimization, particularly when combined with operations like `DynamicUpdateSlice` for constructing the output tensor efficiently.

Overall, TensorFlow models that involve complex, multi-device synchronization of tensors, especially under configurations that might not be directly supported by the underlying hardware or where optimizations can yield significant performance improvements, are likely candidates for this optimization pass.
