### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)


### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `AllReduceCombiner` in TensorFlow XLA.

### Characteristics of TensorFlow Model for `ReshapeReshapeForwarding` Optimization

The `ReshapeReshapeForwarding` optimization in TensorFlow XLA is triggered when there is a specific pattern of consecutive reshape operations in the model. The characteristics of the TensorFlow model that would trigger this optimization are:

1. **Consecutive Reshape Operations**: The model must include two consecutive `reshape` operations. The output of the first `reshape` operation acts as the input to the second `reshape` operation.

2. **Matching Input and Output Shapes**: The input tensor of the first `reshape` operation and the output tensor of the second `reshape` operation must have the same shape. This implies that the transformations applied by the two `reshape` operations effectively cancel each other out.

3. **Non-trivial Transformation**: The reshapes between the two operations are non-trivial, meaning that the intermediate shape between the two reshapes is different from both the initial and final shapes, although the net effect is an identity operation over the tensor shape.

**Illustrative Code Example**:
```python
import tensorflow as tf

# Input tensor of shape [M, N]
input_tensor = tf.random.normal([M, N])

# First reshape operation changing shape to some intermediate shape
t1 = tf.reshape(input_tensor, [X, Y])

# Second reshape operation reverting shape back to original
t2 = tf.reshape(t1, [M, N])
```
In this scenario, despite the intermediate shape change, the final shape of `t2` matches the original `input_tensor` shape, triggering the optimization.

### Characteristics of TensorFlow Model for `SliceSinker` Optimization

The `SliceSinker` optimization pass is triggered in models where multiple element-wise operations are performed on slices of tensors that can be optimized by consolidating operations on the unsliced tensors first. Key characteristics include:

1. **Element-wise Operations on Slices**: The model must perform similar element-wise operations (like addition, multiplication) on slices of one or more tensors. 

2. **Consistent Slicing Across Operations**: The slices involved in these operations need to be extracted from the same indices of their respective tensors.

3. **Same Element-wise Operation Across Slices**: The element-wise operations performed on these slices should be the same (e.g., all additions).

4. **Potential for Reduced Operation Cost**: The optimization checks if performing the operation on the entire tensor before slicing (post-operation) is cheaper in terms of computation compared to performing operations on slices individually.

**Illustrative Code Example**:
```python
import tensorflow as tf

# Input tensor
input_tensor = tf.random.normal([100])

# Slicing the tensor and performing element-wise operations
slice_a = input_tensor[:50]
slice_b = input_tensor[50:]

result_a = slice_a * slice_a  # Element-wise multiplication on first slice
result_b = slice_b * slice_b  # Element-wise multiplication on second slice
```
In this example, the model performs the same operation (multiplication) on two slices of the same tensor. The `SliceSinker` optimization can potentially transform this to operate on the entire tensor first and then slice the result, which could be less expensive computationally.

These characteristics define when each respective optimization would be applied, leading to potential performance improvements in the execution of TensorFlow models on XLA.