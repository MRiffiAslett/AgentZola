### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)




### Characteristics of TensorFlow models that trigger the `ReshapeReshapeForwarding` optimization in TensorFlow XLA

To trigger the `ReshapeReshapeForwarding` optimization, the TensorFlow model should contain a sequence of two `tf.reshape` operations where:

1. The first `tf.reshape` operation reshapes an input tensor `input_tensor` into any arbitrary shape.
2. The second `tf.reshape` operation subsequently reshapes the output of the first reshape back to the original shape of `input_tensor`.

Here is a code example that illustrates this pattern:

```python
import tensorflow as tf

# Original input tensor
input_tensor = tf.random.normal([8, 8])

# First reshape into any new shape
t1 = tf.reshape(input_tensor, [64])

# Second reshape that reverses the first reshape
t2 = tf.reshape(t1, tf.shape(input_tensor))

# t2 will effectively have the same shape and content as input_tensor
```

In this model, the `ReshapeReshapeForwarding` optimization would recognize that the second reshape inverts the first, and the optimization would simplify these operations by eliminating both reshapes, directly using `input_tensor` instead.

### Characteristics of TensorFlow models that trigger the `SliceSinker` optimization in TensorFlow XLA

The `SliceSinker` optimization is triggered when:

1. Multiple elementwise operations are performed on slices of tensors where these slices are taken from the same indices of tensors with compatible shapes.
2. The operations are similar (same operation type and processing similar slices).
3. Combining these operations into a single operation on the unsliced tensors followed by slicing the result is more efficient than performing the operations separately on the slices.

Here is a code example to illustrate this pattern:

```python
import tensorflow as tf

# Original tensor
p = tf.random.normal([10])

# Different slices on the same tensor, followed by similar elementwise operations
a = p[0:9]  # First slice
aa = a + a  # Operation on first slice

b = p[2:10] # Overlapping slice
bb = b + b  # Similar operation on the overlapping slice

# Potential reorganization by SliceSinker:
# A single addition on the whole tensor followed by slicing the result
full_add = p + p
aa_new = full_add[0:9]
bb_new = full_add[2:10]
```

In this example, the `SliceSinker` optimization would recognize that `aa` and `bb` are performing the same operation (`+`) on overlapping slices of the same tensor `p`. It would then merge these operations into a single operation on the entire tensor, followed by extracting the necessary slices from the result. This avoids redundant computations and can lead to performance improvements.