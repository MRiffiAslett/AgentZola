### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)




### Characteristics of TensorFlow Models for `ReshapeReshapeForwarding`

The TensorFlow XLA optimization pass `ReshapeReshapeForwarding` will be triggered under the following model characteristics:

1. **Sequential Reshape Operations:** The model must contain a sequence of two reshape operations. The first reshape operation transforms a tensor `input_tensor` from its original shape to any new shape. The second reshape operation then transforms the tensor back to the original shape of `input_tensor`.

2. **Shape Compatibility:** The output shape of the second reshape operation must exactly match the shape of the tensor input to the first reshape operation. This ensures that the net transformation of the sequence of reshape operations is effectively a no-op.

**Example:**
```python
import tensorflow as tf

# Define an input tensor of shape (10, 20)
input_tensor = tf.random.normal([10, 20])

# First reshape operation (any arbitrary new shape)
t1 = tf.reshape(input_tensor, [200])

# Second reshape operation (back to the original shape)
t2 = tf.reshape(t1, [10, 20])
```
In this example, `t2` would be eligible for optimization to directly reference `input_tensor` without any intermediate reshaping, assuming it matches the original tensor's shape.

### Characteristics of TensorFlow Models for `AllReduceReassociate`

The TensorFlow XLA optimization pass `AllReduceReassociate` will be triggered under the following model characteristics:

1. **Adjacent AllReduce Operations:** The model must contain two adjacent all-reduce operations which are part of an expression to be reassociated. These operations should ideally be in a pattern where reassociation can simplify the all-reduce operations into a single, more efficient operation.

2. **Compatible Reduction Operations:** The all-reduce operations must use compatible reduction functions (e.g., SUM, MAX). This compatibility is essential for the possibility of merging these operations into a single all-reduce operation.

3. **Shape and Type Compatibility:** The data types and shapes involved in the all-reduce operations should be compatible. Additionally, conditions like identical dynamic slice patterns or compatible convert operations (if present) should be met.

4. **Identity Element Presence:** The all-reduce operations should have a defined identity element for the reduction operation being performed. This identity element is crucial for ensuring that the reassociation does not alter the computational semantics.

**Example:**
```python
import tensorflow as tf

# Define tensors
x = tf.random.normal([1024])
y = tf.random.normal([1024])

# First AllReduce operation
ar_x = tf.raw_ops.AllReduce(input=x, reduction="Add", group_size=1)

# Second AllReduce operation
ar_y = tf.raw_ops.AllReduce(input=y, reduction="Add", group_size=1)

# Combining operations
result = ar_x + ar_y
```
In this example, if `ar_x` and `ar_y` are compatible in terms of reduction types and other attributes, and if the addition operation can be reassociated effectively, the `AllReduceReassociate` optimization might merge `ar_x` and `ar_y` into a single all-reduce operation, improving efficiency.