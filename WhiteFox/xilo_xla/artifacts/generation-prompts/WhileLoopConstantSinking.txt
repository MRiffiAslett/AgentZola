### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)




### Characteristics of TensorFlow Model for `ReshapeReshapeForwarding` Optimization

The `ReshapeReshapeForwarding` optimization pass in TensorFlow XLA is triggered when there exists a sequence of two `reshape` operations on a tensor where the first reshape operation changes the shape of a tensor `A` to any other shape, and the subsequent reshape operation changes it back to the original shape of `A`. This optimization pass effectively recognizes and eliminates these redundant reshape operations.

**Model Characteristics:**
- There must be two consecutive `reshape` operations.
- The output shape of the first `reshape` operation and the input shape of the second `reshape` operation can be any shape, as long as the output shape of the second `reshape` matches the input shape of the first one.

**Code Example:**
```python
import tensorflow as tf

# Input tensor
input_tensor = tf.range(9)

# First reshape operation
t1 = tf.reshape(input_tensor, [3, 3])

# Second reshape operation that reverses the first reshape
t2 = tf.reshape(t1, tf.shape(input_tensor))

# t2 will effectively be the same as input_tensor after optimization
```

In this example, `t1` is reshaped to a 3x3 matrix, and `t2` reshapes it back to its original shape. The `ReshapeReshapeForwarding` optimization would recognize this pattern and eliminate the redundant reshapes, directly using `input_tensor` wherever `t2` is used.

### Characteristics of TensorFlow Model for `WhileLoopConstantSinking` Optimization

The `WhileLoopConstantSinking` optimization pass is triggered when constants (or broadcasts of constants) used in the body or condition of a TensorFlow XLA while loop can be "sunk" into the loop. This can reduce redundant computations inside the loop by turning variables that do not change across iterations into constants.

**Model Characteristics:**
- The model must contain a `while` loop.
- Inside the `while` loop, there must be `GetTupleElement` (GTE) operations that are loop-invariant, i.e., their values do not change across iterations of the loop.
- These invariant values must either be constants or broadcasts of constants. Optionally, if the `sink_only_scalar_constants_` flag is set, these constants must be scalars.

**Code Example:**
```python
import tensorflow as tf

# Example of a TensorFlow model with a while loop where constant sinking could be applied
i = tf.constant(0)
max_iter = tf.constant(10)
constant_val = tf.constant(3.0)

def cond(i, val):
    return i < max_iter

def body(i, val):
    # Operations using constant_val which remains unchanged in the loop
    new_val = val * constant_val  
    return i + 1, new_val

# The while loop
result = tf.while_loop(cond, body, [i, constant_val])

# constant_val is used in a loop-invariant manner
```

In this model, `constant_val` is used inside the loop but does not change across iterations. The `WhileLoopConstantSinking` optimization would recognize this and optimize the loop by treating `constant_val` as a constant within the loop body and condition, reducing the computational overhead and potentially memory usage.