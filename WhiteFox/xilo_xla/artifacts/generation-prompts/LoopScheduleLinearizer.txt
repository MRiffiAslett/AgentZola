### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)




### Characteristics of TensorFlow Model for `ReshapeReshapeForwarding` Optimization

To trigger the `ReshapeReshapeForwarding` optimization in TensorFlow XLA, a TensorFlow model should include a specific sequence of reshape operations. Specifically, the model needs a scenario where an input tensor undergoes two consecutive reshape operations, where the second reshape operation reverses the shape transformation applied by the first reshape operation, effectively restoring the tensor to its original shape.

#### Pattern Example:
```python
import tensorflow as tf

# Define an input tensor of any shape
input_tensor = tf.random.normal([8, 16])

# First reshape operation that changes the shape
t1 = tf.reshape(input_tensor, [2, 64])

# Second reshape operation that restores the original shape
t2 = tf.reshape(t1, [8, 16])
```
In this example, `t1` is reshaped from `[8, 16]` to `[2, 64]` and `t2` is then reshaped back to `[8, 16]`. This pattern will trigger the optimization, and the optimizer will recognize that `t2` can be directly replaced by `input_tensor`, simplifying the computation graph.

### Characteristics of TensorFlow Model for `LoopScheduleLinearizer` Optimization

For the `LoopScheduleLinearizer` optimization to be triggered, the TensorFlow model must involve a while-loop (`tf.while_loop` in TensorFlow) where there is an opportunity to introduce additional control dependencies to linearize the execution schedule within the loop body. This optimization targets scenarios where adding control dependencies between loop body operations can prevent potential data hazards and improve execution determinism.

#### Conditions Requiring Optimization:
1. The model contains at least one `tf.while_loop`.
2. The loop body does not involve asynchronous collective operations which could be affected negatively by additional control dependencies.
3. There are opportunities within the loop body to add control dependencies between operations, particularly where such dependencies can enforce a more linear execution order without introducing cycles in the computation graph.

#### Example Pattern:
```python
i = tf.constant(0)
c = lambda i: tf.less(i, 10)

def body(i):
    # Simulate data dependency
    a = tf.multiply(i, 2)
    b = tf.add(a, 1)
    return tf.add(i, 1)

# TensorFlow while loop
result = tf.while_loop(c, body, [i])
```
In this example, if the model's data flow analysis reveals that the operation `b = tf.add(a, 1)` could benefit from a control dependency to ensure it occurs after `a = tf.multiply(i, 2)` consistently for better performance or correctness, the `LoopScheduleLinearizer` might add such a dependency.

This optimization is particularly beneficial in complex loops where the order of operations impacts performance or where data dependencies are critical to maintaining correct computation results.