### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)


### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `AllReduceCombiner` in TensorFlow XLA.

### Characteristics of TensorFlow Model for `ReshapeReshapeForwarding` Optimization

The `ReshapeReshapeForwarding` optimization in TensorFlow XLA is triggered when a TensorFlow model contains a sequence of two `reshape` operations where:

1. The first `reshape` operation changes the shape of a tensor `input_tensor` to any new shape.
2. The second `reshape` operation directly consumes the output of the first `reshape` and reverts it back to the original shape of `input_tensor`.

This pattern implies that the output shape of the second `reshape` matches the shape of the tensor input to the first `reshape`. Essentially, the sequence of reshape operations is redundant as they transform the tensor into a new shape and then immediately revert to the original shape.

#### Illustrative Code Example:
```python
import tensorflow as tf

# Sample input tensor
input_tensor = tf.constant([[1, 2], [3, 4]])

# First reshape operation: Change shape
reshaped_tensor = tf.reshape(input_tensor, [4])

# Second reshape operation: Revert to original shape
reverted_tensor = tf.reshape(reshaped_tensor, input_tensor.shape)

# reverted_tensor should now have the same shape and contents as input_tensor
```
In this example, the first reshape changes the shape to `[4]`, and the second reshape reverts it back to `[2, 2]`, which is the original shape of `input_tensor`. This sequence triggers the `ReshapeReshapeForwarding` optimization pass, where the two reshapes can be removed, directly using `input_tensor` instead.

### Characteristics of TensorFlow Model for `ZeroSizedHloElimination` Optimization

The `ZeroSizedHloElimination` optimization pass is triggered when the TensorFlow XLA model contains operations that produce zero-sized arrays. This means the tensor has a shape where at least one dimension has a size of zero, resulting in a total element count of zero.

#### Conditions for triggering the optimization:
1. The operation produces an array-shaped tensor with zero elements.
2. The operation should not have any side effects that would prevent its removal, except for specific custom operations that can be replaced even if they typically have side effects.
3. The tensor should have a static shape, meaning its dimensions are known and fixed at compile time.
4. The operation is not a `kConstant` operation since constants are already optimized in their representation.

#### Illustrative Code Example:
```python
import tensorflow as tf

# Zero-sized tensor due to one dimension being zero
zero_sized_tensor = tf.zeros([0, 10])

# Some operations performed on zero-sized tensor
result_tensor = zero_sized_tensor + 5  # This tensor is also zero-sized

# Another example with reshaping
reshaped_tensor = tf.reshape(zero_sized_tensor, [0, 5, 2])  # Still zero-sized
```
In these examples, since the operations produce zero-sized arrays and meet the conditions listed above, they trigger the `ZeroSizedHloElimination` optimization. This optimization can potentially remove these operations or replace them with more efficient constructs, as they do not contribute meaningful computational load or output due to their zero-sized nature.