### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)




### Characteristics of TensorFlow Model Triggering `ReshapeReshapeForwarding`
For the `ReshapeReshapeForwarding` optimization in TensorFlow XLA to be triggered, the TensorFlow model must contain two consecutive `tf.reshape` operations where the output shape of the second reshape operation matches the input shape of the tensor fed into the first reshape operation. This specific sequence effectively reverses the first reshape operation, rendering the sequence redundant.

**Illustrative TensorFlow Code:**
```python
import tensorflow as tf

input_tensor = tf.random.normal([8, 8])
# First reshape
t1 = tf.reshape(input_tensor, [64])
# Second reshape that reverses the first
t2 = tf.reshape(t1, [8, 8])
```
In this example, `input_tensor` has an initial shape of `[8, 8]`, which is reshaped to `[64]` by `t1` and then reshaped back to `[8, 8]` by `t2`. This scenario matches the pattern detected by the `ReshapeReshapeForwarding` optimization.

### Characteristics of TensorFlow Model Triggering `ShardingRemover`
The `ShardingRemover` optimization in TensorFlow XLA is activated when the model contains specific custom-call operations related to tensor sharding and reshaping. These operations are generally used in SPMD (Single Program, Multiple Data) partitioning strategies and include:

- `Sharding`
- `SPMDShardToFullShape`
- `SPMDFullToShardShape`
- `ShardingGroupCustomCallTargetName`
- `PropagationBarrierCustomCallTargetName`
- `FuncResultShardingTargetName`
- `ShardBarrierFrom`
- `ShardBarrierTo`

The optimization is triggered if any of these operations are present in the model and are effectively redundant, meaning they can be replaced by direct references to their operand tensors without loss of function or incorrect behavior.

**Illustrative TensorFlow Code (Hypothetical):**
```python
# Example of a custom-call operation that could trigger ShardingRemover
# Note: This is a conceptual illustration since TensorFlow Python API does not expose these low-level operations directly.
sharded_tensor = some_custom_sharding_operation(input_tensor)
restored_tensor = spmd_shard_to_full_shape(sharded_tensor)
```
Assuming `some_custom_sharding_operation` and `spmd_shard_to_full_shape` are placeholder names for operations that perform sharding and then reverse it, this pattern is what `ShardingRemover` seeks to optimize by removing the sharding operations and directly using `input_tensor` wherever `restored_tensor` is used.

In both cases, the optimizations are focused on simplifying the computational graph by removing operations that are redundant or can be substituted without altering the model's output.