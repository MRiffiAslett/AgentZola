### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)




### Characteristics of TensorFlow models triggering `Bfloat16ConversionFolding` optimization in TensorFlow XLA

The `Bfloat16ConversionFolding` optimization is triggered in TensorFlow models where specific operations involving BF16 (bfloat16) and F32 (float32) data types occur. This optimization specifically targets scenarios where unnecessary data type conversions between BF16 and F32 can be folded into the operations themselves, optimizing the model for performance and potentially reducing memory usage. Here are the key characteristics and patterns in TensorFlow models that would trigger this optimization:

1. **Presence of BF16 to F32 Conversions as Operation Inputs:**
   If an operation has an input that is a conversion from BF16 to F32, and the operation itself supports BF16 inputs directly, this optimization will fold the conversion into the operation. This is applicable when the operation's precision support allows such a mixed precision or when all inputs can be uniformly treated as BF16.

   ```python
   # Example TensorFlow Code
   input_bf16 = tf.cast(input_tensor, tf.bfloat16)
   converted_f32 = tf.cast(input_bf16, tf.float32)
   result = tf.some_operation(converted_f32)  # some_operation supports BF16 directly
   ```

2. **Presence of F32 to BF16 Conversions as Operation Outputs:**
   If all users of an operation’s output are converting the output from F32 to BF16, and the operation supports BF16 output, this optimization will fold these conversions by changing the operation’s output directly to BF16.

   ```python
   # Example TensorFlow Code
   result_f32 = tf.some_operation(input_tensor)  # some_operation supports BF16 output
   result_bf16 = tf.cast(result_f32, tf.bfloat16)
   ```

3. **Operations with Mixed Precision Support:**
   If an operation supports mixed precision, where some inputs or the output are in BF16 and others are in F32, this optimization can selectively fold conversions where feasible.

4. **Absence of Side-Effects or Specific Instruction Constraints:**
   The optimization avoids folding conversions for operations that are side-effecting, involve tuples, or are critical for control flow (like `tf.while_loop`). Also, operations like `tf.convert_to_tensor` or those involving fusion, bitcasting, or conditions are excluded.

5. **Computation Root Conditions:**
   If the operation is the root of the computation, the optimization will be conservative unless the operation supports mixed precision, ensuring the output type remains unchanged unless all associated conditions are met.

By optimizing away unnecessary data type conversions, this pass helps in enhancing computational efficiency and reducing runtime overhead in models extensively using BF16 and F32 data types, especially in machine learning models where low precision arithmetic is sufficient for training and inference tasks.