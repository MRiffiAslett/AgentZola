### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `AllReduceSimplifier` in TensorFlow XLA.

# Description
The TensorFlow XLA optimization `AllReduceSimplifier` targets specific characteristics in a TensorFlow model's distributed training setup. Here are the main characteristics that trigger this optimization, causing the function to return `true`:

1. **AllGather or ReduceScatter with Compatible Shapes**: The model contains either AllGather or ReduceScatter operations where the input and output shapes are identical. This pattern suggests redundancy, as reshaping without dimension change doesn't require any actual data movement or computation.

   Example:
   ```python
   x = tf.random.uniform([10])
   y = tf.raw_ops.AllGather(input=x, group_size=1)  # Output shape matches input shape.
   ```

2. **AllReduce Operations with Simplifiable Conditions**:
   - The AllReduce must be either a cross-replica or a cross-module operation.
   - All replica groups participating in the AllReduce operation must have the same size. 
   - Specific optimizations are applied based on the operation within the AllReduce:
     - If the operation inside AllReduce is addition (`Add`), the optimization might transform it by multiplying the input by the size of the replica group (given certain conditions on element types and shapes).
     - For operations like `Minimum`, `Maximum`, `Or`, and `And`, the input can directly replace the AllReduce operation if all inputs are already identical across replicas or if the replica group size is 1.

   Example:
   ```python
   x = tf.random.uniform([10])
   y = tf.raw_ops.AllReduce(input=x, reduction="Add", group_size=10)
   # This could be optimized to multiply `x` by 10 directly if other conditions are met.
   ```

3. **Replication Analysis Compliance**: The optimization checks if the input to the AllReduce operation is replicated identically across different replicas or if the group size equals 1. This avoids unnecessary collective operations when the result is predetermined by the input distribution.

The optimization can significantly simplify operations, reducing computational overhead and communication across devices in distributed setups. Models with redundant or simplifiable collective operations, especially in large-scale training scenarios, are likely to benefit from this optimization pass.
