### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)


### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `AllReduceCombiner` in TensorFlow XLA.

### Characteristics of TensorFlow Model for `CollectivesScheduleLinearizer` Optimization

In TensorFlow XLA, the `CollectivesScheduleLinearizer` optimization pass is triggered for models that utilize collective operations (like `AllReduce`, `AllGather`, `CollectivePermute`, etc.) in an asynchronous or non-linear order, potentially across multiple threads of execution. The characteristic pattern involves multiple such collective operations where the execution order is not explicitly controlled or where dependencies between these operations are not straightforward.

This optimization will apply when:
1. The model contains collective operations that can be asynchronous, such as starting a collective operation (`AllReduceStart`, `AllGatherStart`, etc.) and then having a corresponding `AsyncDone`.
2. These collective operations are not strictly sequentially connected in their execution paths, meaning there's no direct control dependency between successive collective operations.

The optimization pass intervenes to linearize the schedule of these operations by adding explicit control dependencies. This ensures that collective operations that could potentially execute out of order now execute in a linear, predictable manner.

#### Code Example for Pattern

Hereâ€™s a simple TensorFlow code snippet that could potentially trigger this optimization if converted to HLO and optimized by XLA:

```python
import tensorflow as tf

# Example tensors
tensor1 = tf.random.normal([256, 256])
tensor2 = tf.random.normal([256, 256])

# Start of asynchronous collective operations
all_reduce_start1 = tf.raw_ops.AllReduce(input=tensor1, reduction="Add", group_size=1, group_key=1, instance_key=1)
all_reduce_start2 = tf.raw_ops.AllReduce(input=tensor2, reduction="Add", group_size=1, group_key=1, instance_key=2)

# Corresponding AsyncDone operations would typically be managed within TensorFlow's runtime
```

In this example, `all_reduce_start1` and `all_reduce_start2` are collective operations that start asynchronously. In a more complex computation graph, without explicit control dependencies or linear execution paths, the `CollectivesScheduleLinearizer` could intervene to add control dependencies to ensure that these operations execute in a deterministic order.