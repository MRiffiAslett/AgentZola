### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)




### Characteristics of TensorFlow Model for Optimization Pass `CollectivesScheduleLinearizer`

The `CollectivesScheduleLinearizer` optimization pass is triggered for TensorFlow XLA models containing the following characteristics:

1. **Presence of Collective Operations:** The model should contain collective operations such as `AllReduce`, `AllGather`, `CollectivePermute`, or their asynchronous variants (`AllReduceStart`, `AllGatherStart`, `CollectivePermuteStart`, etc.). These operations are crucial as the optimization targets the linearization of collectives to ensure correct execution order.

2. **Execution on Specific Threads:** The model's computation must be performed on particular execution threads specified by the `execution_threads` parameter, which filters the computations that this pass applies to.

3. **Independent Collective Operations:** The optimization is triggered when there are multiple collective operations in sequence, where the subsequent operation does not depend on the completion of the previous one (i.e., they are not already connected in the reachability map). The optimization inserts control dependencies to enforce a specific linear execution order.

Here is an illustrative example:

```python
import tensorflow as tf

# Assuming a distributed setup with multiple devices
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    # Dummy input tensor
    input_tensor = tf.random.normal([10, 10])

    # Collective operations
    sum_reduced = tf.raw_ops.AllReduce(input_tensor, reduction="Add", group_assignment=[[0, 1], [2, 3]])
    max_reduced = tf.raw_ops.AllReduce(input_tensor, reduction="Max", group_assignment=[[0, 1], [2, 3]])

    # The model must be constructed in such a way that these operations are considered independent
    # and thus the optimization pass would trigger to add control dependencies ensuring correct order.
```

In this example, if the `sum_reduced` and `max_reduced` operations are found to be independent in the HLO module's computation, and they aren't already ordered by data dependencies, the `CollectivesScheduleLinearizer` would add a control dependency between them. This ensures that `sum_reduced` operation completes before `max_reduced` starts, maintaining a linear and predictable execution order for collective operations.